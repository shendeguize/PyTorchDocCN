# 自动求导机制

本文会概览自动梯度求导是如何工作和记录操作的.实际上并不是严格必要地理解全部内容,但是我们建议要熟悉他,因为这会有利于你写出更高效也更简明的程序并且帮助你debug.

## 从反向传播中排除子图
每个`Tensor`都有一个标签:`requires_grad`,这个标签使得我们能够细粒度地在计算梯度的过程中排除子图,并提升性能.

###  `requires_grad`
如果有对于一个运算有一个需要梯度的单独输入,那么对应的输出也需要梯度.与此相对,如果所有的输入都不需要梯度,那么输出也不需要.在所有的Tensor都不需要梯度时,反向传播计算是不会在子图中运算.

```Python3
>>> x = torch.randn(5, 5)  # requires_grad=False by default 默认为False
>>> y = torch.randn(5, 5)  # requires_grad=False by default
>>> z = torch.randn((5, 5), requires_grad=True)
>>> a = x + y
>>> a.requires_grad
False
>>> b = a + z
>>> b.requires_grad
True
```
对于你希望固定你的模型一部分时或者你已经知道一些参数是不需要参数的情况下这一方式尤其有效.例如你希望调整一个预训练的CNN时,对于固定的基础部分把`requires_grad`置为False就足够了,直到计算到最后一层需要保留梯度的权重的网络层前,中间计算缓存是不会被保留的.而最终网络的输出也是需要保留梯度的.

```Python3
model = torchvision.models.resnet18(pretrained=True)
for param in model.parameters():
    param.requires_grad = False
# Replace the last fully-connected layer
# Parameters of newly constructed modules have requires_grad=True by default
# 替换最后一层
# 新建Module的参数默认需要保留梯度
model.fc = nn.Linear(512, 100)

# Optimize only the classifier
optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)
```
# TBD






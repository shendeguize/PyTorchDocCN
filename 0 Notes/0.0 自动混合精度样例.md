# 自动混合精度样例
source: https://pytorch.org/docs/stable/notes/amp_examples.html#working-with-scaled-gradients

version: 1.5.0

date: 2020.05.17

author: shendeguize@github

***警告***  
[`torch.cuda.amp.GradScaler`](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler)
并非自动混合精度的完整实现.只有在手动将模型的局部以`float16`运行时[`GradScaler`](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler)才有用.如果你不确定如何手动选择运算精度.master分支以及nightly版本pip/conda源包含了能够自动选择运算精度的上下文管理器.具体请查看[master文档](https://pytorch.org/docs/master/amp.html).

**目录**  
[TOC]


## 梯度缩放
梯度缩放能够帮助避免在混合精度训练中出现的梯度下溢出,[具体信息](https://pytorch.org/docs/stable/amp.html#gradient-scaling).  
[`torch.cuda.amp.GradScaler`](https://pytorch.org/docs/stable/amp.html#torch.cuda.amp.GradScaler)的实例能够方便的实现梯度缩放的step,具体如下述样例.  
### 典型应用

```Python
# Creates a GradScaler once at the beginning of training.
# 在训练开始时实例化一个GradScaler
scaler = GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)

        # Scales the loss, and calls backward() on the scaled loss to create scaled gradients.
		# 缩放loss,并调用backward(),基于缩放后的loss获取缩放后梯度
        scaler.scale(loss).backward()

        # scaler.step() first unscales the gradients of the optimizer's assigned params.
		# scaler.step()首先逆缩放优化器参数组的梯度
        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,
		# 如果这些梯度不包含inf或者NaN,就会正常调用optimizer.step()
        # otherwise, optimizer.step() is skipped.
		# 否则将跳过optimizer.step()
        scaler.step(optimizer)

        # Updates the scale for next iteration.
		# 更新下个迭代周期的缩放
        scaler.update()
```

### 使用未缩放梯度
经由`scaler.scale(loss).backward()`得到的所有梯度都是缩放后的.如果希望在`backward()`和`scale.step(optimizer)`之间能够调整或者查看参数的`.grad`属性,那么首先应该逆缩放.例如,梯度的截断操作了一组梯度使其全局范数(参见[` torch.nn.utils.clip_grad_norm_()`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_norm_)或最大量级(参见[`
torch.nn.utils.clip_grad_value_()`](https://pytorch.org/docs/stable/nn.html#torch.nn.utils.clip_grad_value_)小于等于用户外部定义的阈值.如果你试图不经过逆缩放就进行阶段,梯度的范数或者最大量级也将是缩放后的结果,从而使得施加的阈值(指原本要应用于逆缩放后梯度的阈值)约束失效.


`scaler.unscale_(optimizer)`能够将`optimizer`分配的参数的梯度逆缩放.如果模型包含被分配给其他优化器(此处记为`optimizer2`)参数,也可独立调用`scaler.unscale_(optimizer2)`来逆缩放这些参数的梯度.

#### 梯度截断
在截断前调用`scaler.unscale_(optimizer)`以和对正常未缩放梯度同样的方式进行截断.
```python
scaler = GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        scaler.scale(loss).backward()

        # Unscales the gradients of optimizer's assigned params in-place
		# 原地逆缩放优化器参数的梯度
        scaler.unscale_(optimizer)

        # Since the gradients of optimizer's assigned params are unscaled, clips as usual:
		# 由于梯度已经被逆缩放,正常截断
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)

        # optimizer's gradients are already unscaled, so scaler.step does not unscale them,
		# 优化器梯度已经被逆缩放过,因此scaler.step不会再次逆缩放
        # although it still skips optimizer.step() if the gradients contain infs or NaNs.
		# 尽管如此,如果梯度包含inf或者NaN,仍旧会跳过optimizer.step()
        scaler.step(optimizer)

        # Updates the scale for next iteration.
        scaler.update()
```

`scaler`记录了`scaler.unscale_(optimizer)`在这个迭代周期内已经被调用,所以` scaler.unscale_(optimizer)`不会重复逆缩放梯度直到内部调用`optimizer.step()`.

***警告***  
`unscale_()`在每个优化器的每个`step()`调用中被调用一次.并且只能在对应优化器参数的所有梯度都已获取的情况下被调用.对于某一优化器的两次`step()`之间调用两次`unscale_()`将抛出`RuntimeError`异常

### 使用缩放后梯度
对于一些运算,可能需要在`scaler.unscale_`不适用的环境下处理缩放梯度.

#### 梯度罚项
梯度罚项的实现通过[`torch.autograd.grad()`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad)创建out-of-place梯度,结合梯度值求得罚项值,并加入loss中.

这里是一个无缩放梯度L2罚项的例子:

```python
for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)

        # Creates some gradients out-of-place
        grad_params = torch.autograd.grad(loss, model.parameters(), create_graph=True)

        # Computes the penalty term and adds it to the loss
        grad_norm = 0
        for grad in grad_params:
            grad_norm += grad.pow(2).sum()
        grad_norm = grad_norm.sqrt()
        loss = loss + grad_norm

        loss.backward()
        optimizer.step()
```

为了实现缩放梯度的罚项,传递给[`torch.autograd.grad()`](https://pytorch.org/docs/stable/autograd.html#torch.autograd.grad)的loss也用被缩放.故而out-of-place梯度也要被缩放,并在生成罚项之前就被缩放.

```python

for epoch in epochs:
    for input, target in data:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)

        # Scales the loss for the out-of-place backward pass, resulting in scaled grad_params
        scaled_grad_params = torch.autograd.grad(scaler.scale(loss), model.parameters(), create_graph=True)

        # Unscales grad_params before computing the penalty.  grad_params are not owned
        # by any optimizer, so ordinary division is used instead of scaler.unscale_:
        inv_scale = 1./scaler.get_scale()
        grad_params = [p*inv_scale for p in scaled_grad_params]

        # Computes the penalty term and adds it to the loss
        grad_norm = 0
        for grad in grad_params:
            grad_norm += grad.pow(2).sum()
        grad_norm = grad_norm.sqrt()
        loss = loss + grad_norm

        # Applies scaling to the backward call as usual.  Accumulates leaf gradients that are correctly scaled.
        scaler.scale(loss).backward()

        # step() and update() proceed as usual.
        scaler.step(optimizer)
        scaler.update()
```

### 处理多loss和多个优化器
如果你的网络有多个loss,需要对每个loss单独使用`scaler.scale`.如果网络有多个优化器,可以对任意优化器单独调用`scaler.unscale_`,也必须对每一个单独调用`scaler.stap`

不管怎样,`scaler.update()`应该只被调用一次,在这个迭代周期中的所有优化器执行完step.
```python
scaler = torch.cuda.amp.GradScaler()

for epoch in epochs:
    for input, target in data:
        optimizer0.zero_grad()
        optimizer1.zero_grad()
        output0 = model0(input)
        output1 = model1(input)
        loss0 = loss_fn(2 * output0 + 3 * output1, target)
        loss1 = loss_fn(3 * output0 - 5 * output1, target)

        scaler.scale(loss0).backward(retain_graph=True)
        scaler.scale(loss1).backward()

        # You can choose which optimizers receive explicit unscaling, if you
        # want to inspect or modify the gradients of the params they own.
        scaler.unscale_(optimizer0)

        scaler.step(optimizer0)
        scaler.step(optimizer1)

        scaler.update()
```

每个优化器独立的检查梯度是否包含inf或者NaN,切独立决定是否跳过当前step.这可能导致一个优化器跳过了step而其他优化器没有跳过.因为跳过step非常少出现(没几百轮迭代),这不会影响收敛.如有发现在多优化器模型加上梯度缩放后收敛结果很差,清提交issue.
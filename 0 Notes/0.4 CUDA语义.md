# CUDA语义
`torch.cuda`用于设定和运行CUDA操作.持续跟随当前选中的GPU,所有声明的CUDA Tensor都会默认在该设备商创建.所选设备可以被`torch.cuda.device`上下文管理器修改.

然而一旦一个Tensor被声明,可以在所选设备上进行运算,结果也会被置于与Tensor相同的设备上.

夸GPU操作默认是不允许的,除非使用`copy_()`和其他类似于拷贝功能的方法诸如`to()`和`cuda()`.除非启用peer-to-peer内存访问,任何在不同设备间的Tensor上进行运算的尝试都会报错.

下述是一个小例子:

```Python
cuda = torch.device('cuda')     # Default CUDA device
cuda0 = torch.device('cuda:0')
cuda2 = torch.device('cuda:2')  # GPU 2 (these are 0-indexed)

x = torch.tensor([1., 2.], device=cuda0)
# x.device is device(type='cuda', index=0)
y = torch.tensor([1., 2.]).cuda()
# y.device is device(type='cuda', index=0)

with torch.cuda.device(1):
    # allocates a tensor on GPU 1
    a = torch.tensor([1., 2.], device=cuda)

    # transfers a tensor from CPU to GPU 1
    b = torch.tensor([1., 2.]).cuda()
    # a.device and b.device are device(type='cuda', index=1)

    # You can also use ``Tensor.to`` to transfer a tensor:
    b2 = torch.tensor([1., 2.]).to(device=cuda)
    # b.device and b2.device are device(type='cuda', index=1)

    c = a + b
    # c.device is device(type='cuda', index=1)

    z = x + y
    # z.device is device(type='cuda', index=0)

    # even within a context, you can specify the device
    # (or give a GPU index to the .cuda call)
    d = torch.randn(2, device=cuda2)
    e = torch.randn(2).to(cuda2)
    f = torch.randn(2).cuda(cuda2)
    # d.device, e.device, and f.device are all device(type='cuda', index=2)
```

## 
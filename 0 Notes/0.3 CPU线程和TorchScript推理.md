# CPU线程和TorchScript推理
PyTorch允许在使用TorchScript进行推理时使用CPU多线程.下图展示了在应用中会使用的不同级别的并行.

![img](https://github.com/shendeguize/PyTorchDocCN/blob/master/_imgs/0.3_0.png)

某一或者更多个线程执行某一个输入进入模型的前向操作时.每一个推理线程都会唤起一个即时编译器来逐行执行模型操作.模型可以利用`fork`的TorchScript来启动异步任务.forking若干操作就使得任务是并行执行的.`fork`操作返回的是一个可以被用于后续同步的`Future`对象,例如:
```Python
@torch.jit.script
def compute_z(x):
    return torch.mm(x, self.w_z)

@torch.jit.script
def forward(x):
    # launch compute_z asynchronously:
    # 协程启动compute_z:
    fut = torch.jit._fork(compute_z, x)
    # execute the next operation in parallel to compute_z:
    # 并行地执行下一个操作到compute_z:
    y = torch.mm(x, self.w_y)
    # wait for the result of compute_z:
    # 等待compute_Z的结果:
    z = torch.jit._wait(fut)
    return y + z
```
PyTorch对于操作内的并行使用的是一个单线程池,这个线程池被所有被fork的推理任务在应用过程中共享.

除了运算间并行,PyTorch也在运算内利用了多线程(intra-op并行).这在很多情况下都很有用,包括大张量的位操作,卷积,GEMM(矩阵乘法),embedding lookup(查询)和其他操作.

## build选择
PyTorch使用了内置的ATen库实现运算.此外,PyTorch同样可以基于外部库如[MKL](https://software.intel.com/en-us/mkl)和[MKL-DNN](https://github.com/intel/mkl-dnn)来构建以加速CPU运算.

ATen,MKL和MKL-DNN支持运算内并行,且实现依赖于下述并行库:
+ [OpenMP](https://www.openmp.org/)一个标准(也是一个库,常包含一个编译器),被外部库广泛使用.
+ [TBB](https://github.com/intel/tbb)一个稍新的,优化了基于任务并行和并发环境的库.

OpenMP一直以来被大量库使用.由于其易用性和对于基于循环的并行的支持以及其他特性而知名.

TBB相对而言在外部库中的使用少一些,但同时,对并发环境进行了优化.PyTorch的TBB后端保证了有每个进程的独立的单独运算内线程池被所有运行中的运算使用.

取决于使用情况,可能有其他并行库是更好的选择.

PyTorch允许在build时选择被ATen或其他库使用的并行后端,build选项包括:  
[详见原文档](https://pytorch.org/docs/1.5.0/notes/cpu_threading_torchscript_inference.html#build-options)  
![img](https://github.com/shendeguize/PyTorchDocCN/blob/master/_imgs/0.3_1.png)

建议在一次build中,不要混用OpenMP和TBB.  

上述的所有`TBB`值需要设置`USE_TBB=1`(默认关闭),如需使用OpenMP并行,需要单独设置`USE_OPENMP=1`(模式开启).

## 运行API
下述API用于控制线程设置:   
[详见原文档](https://pytorch.org/docs/1.5.0/notes/cpu_threading_torchscript_inference.html#runtime-api)  
![img](https://github.com/shendeguize/PyTorchDocCN/blob/master/_imgs/0.3_2.png)

对于运算内并行的设置`at::set_num_threads`,`torch.set_num_threads`总是优先于环境变量,`MKL_NUM_THREADS`变量优先于`OMP_NUM_THREADS`.

## 调节线程数
下述简单脚本展示了一个矩阵乘法的运行与线程数的变化关系.

```Python
import timeit
runtimes = []
threads = [1] + [t for t in range(2, 49, 2)]
for t in threads:
    torch.set_num_threads(t)
    r = timeit.timeit(setup = "import torch; x = torch.randn(1024, 1024); y = torch.randn(1024, 1024)", stmt="torch.mm(x, y)", number=100)
    runtimes.append(r)
# ... plotting (threads, runtimes) ...
```

在一台24核CPU(Xeon E5-2680, MKL and OpenMP based build)有下述运行结果:

![img](https://pytorch.org/docs/1.5.0/_images/cpu_threading_runtimes.svg)

在调整运算内/运算间线程数时序考虑下述问题:
+ 当选择线程数时,需要避免过度描述(过多线程会导致性能下降).例如,在一个使用大应用线程池或中毒已炼狱运算间并行的应用中,可能会选择禁用运算内并行(调用`set_num_threads(1)`)
+ 在标准应用中,可能需要平衡*延迟latency*(单次交互请求处理时长)和*吞吐量throughput*(单位时间内完成任务数).调节线程数是个很好的调整这一平衡的工具.例如,在对延迟要求高的应用中,看通过提高运算内线程数来尽可能快的处理每个请求.与此同时,操作的并行可能会有额外损耗导致单次请求的操作量增加进而减少总吞吐量.

:: warn
警告
OpenMP不保证简单每进程的运算内线程池会被利用.相对的,两个应用或运算间线程可能会使用不同的OpenMP运算内线程池.这可能导致应用中存在大量线程被使用.在OpenMP多线程应用中调整线程数时需要格外小心.
::

:: note
预编译的PyTorch发布版是OpenMP支持的.
::

:: note
`parallel_info`功能打印关于线程设置的信息,可被用于调试.相似的输出也可在Python中调用`torch.__config__.parallel_info()`获得.